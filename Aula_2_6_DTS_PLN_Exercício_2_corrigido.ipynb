{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodrigodeFeo/8DTS/blob/main/Aula_2_6_DTS_PLN_Exerc%C3%ADcio_2_corrigido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercício 2 - Aula 2**"
      ],
      "metadata": {
        "id": "b_lUc0-cLsVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dado o dataset de produtos [1], desenvolva os seguintes pipelines:\n",
        "\n",
        "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\n",
        "\n",
        "Obs.: em todos os pipelines use:\n",
        "- normalização renovendo valores faltantes\n",
        "- criem uma nova coluna concatenando as colunas nome e descrição.\n",
        "- randon_state igual a 42 para permitir a comparação com seus colegas e separe uma amostra de 30% para teste.\n"
      ],
      "metadata": {
        "id": "i9jiVrQi-al0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pacotes utilizados\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# carregar dataframe\n",
        "df = pd.read_csv(\"https://dados-ml-pln.s3-sa-east-1.amazonaws.com/produtos.csv\", delimiter=\";\", encoding='utf-8')\n",
        "\n",
        "# limpeza inicial (normalização)\n",
        "df.dropna(inplace=True)\n",
        "df[\"texto\"] = df['nome'] + \" \" + df['descricao']\n",
        "\n",
        "# divisão da amostra entre treino e teste\n",
        "df_train, df_test = train_test_split(\n",
        "      df,\n",
        "      test_size = 0.3,\n",
        "      random_state = 42\n",
        "  )"
      ],
      "metadata": {
        "id": "XjpvUZj5IPpG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. ) Treine um modelo de classificação LogisticRegression do pacote scikit-learn para classificar os produtos em suas categorias, com as seguintes configurações:\n",
        "\n",
        "- 2.1 Contagem de termos simples com unigrama e com stop-words.\n",
        "- 2.2 Contagem de termos simples com unigrama + brigrama e sem stop-words.\n",
        "- 2.3 TF-IDF com unigrama e sem stop-words.\n",
        "- 2.4 TF-IDF com unigrama e sem stop-words em textos lematizados.\n",
        "\n",
        "Extra:\n",
        "- 2.5 Contagem de termos simples (BoW) com unigrama, sem stop-words (combinando Spacy e NLTK) em textos com apenas verbos lematizados.\n",
        "\n",
        "Dica: crie uma função para lematizar o texto usando o Spacy, não esqueça de usar o POS-Tag quando necessário."
      ],
      "metadata": {
        "id": "6I_PEQasSIUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1 Contagem de termos simples com unigrama e com stop-words.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,1))\n",
        "vect.fit(df_train.texto)\n",
        "text_vect_train = vect.transform(df_train.texto)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.texto)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "t7Mx7bXKQjDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59ba003-4625-4683-ad06-81be8b6a9c95"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9805714285714285"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Contagem de termos simples com unigrama + brigrama e sem stop-words.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,2), stop_words=stops)\n",
        "vect.fit(df_train.texto)\n",
        "text_vect_train = vect.transform(df_train.texto)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.texto)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "lQnAI7jfQjA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e3ef60-bb2f-4f38-c616-53e31ef145ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9851428571428571"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 TF-IDF com unigrama e sem stop-words.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops)\n",
        "vect.fit(df_train.texto)\n",
        "text_vect_train = vect.transform(df_train.texto)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "x_test = vect.transform(df_test.texto)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "ZE-yFOCYQi-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "271ca486-8209-46ee-eb95-5a16bb1bac21"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.984"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "r9ELr6DCSdrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d56ff610-2efc-4288-f147-c47163cf2cba"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.0)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.4 TF-IDF com unigrama e sem stop-words em textos lematizados.\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "stops = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# função de lematização completa do documento\n",
        "def lemmatizer_text(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      sent.append(word.lemma_)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_train['text_lemma'] = df_train.texto.apply(lemmatizer_text)\n",
        "\n",
        "# vetorização\n",
        "vect = TfidfVectorizer(ngram_range=(1,1), use_idf=True, stop_words=stops)\n",
        "vect.fit(df_train.text_lemma)\n",
        "text_vect_train = vect.transform(df_train.text_lemma)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_test['text_lemma'] = df_test.texto.apply(lemmatizer_text)\n",
        "x_test = vect.transform(df_test.text_lemma)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "TDNE8CsQSdrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e93df7-c213-44b6-c381-02571761c31f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9828571428571429"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra:\n",
        "# 2.5 Contagem de termos simples (BoW) com unigrama, sem stop-words (combinando Spacy e NLTK) em textos com apenas verbos lematizados.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "nlp = spacy.load('pt_core_news_sm')\n",
        "\n",
        "# stopwords do SpaCy e NLTK combinadas\n",
        "stops = list(set(nlp.Defaults.stop_words).union(set(nltk.corpus.stopwords.words('portuguese'))))\n",
        "\n",
        "# função de lematização para os verbos do documento\n",
        "def lemmatizer_verbs(text):\n",
        "  sent = []\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "      if word.pos_ == \"VERB\":\n",
        "          sent.append(word.lemma_)\n",
        "      else:\n",
        "          sent.append(word.text)\n",
        "  return \" \".join(sent)\n",
        "\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_train['text_lemma_verbs'] = df_train.texto.apply(lemmatizer_verbs)\n",
        "\n",
        "# vetorização\n",
        "vect = CountVectorizer(ngram_range=(1,1), stop_words=stops)\n",
        "vect.fit(df_train.text_lemma_verbs)\n",
        "text_vect_train = vect.transform(df_train.text_lemma_verbs)\n",
        "\n",
        "x_train = text_vect_train\n",
        "y_train = df_train[\"categoria\"]\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# treinamento do modelo\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(x_train, y_train)\n",
        "\n",
        "# teste do modelo\n",
        "# aplica a lematização no dataframe de treino criando um nova coluna\n",
        "df_test['text_lemma_verbs'] = df_test.texto.apply(lemmatizer_verbs)\n",
        "x_test = vect.transform(df_test.text_lemma_verbs)\n",
        "y_test = df_test[\"categoria\"]\n",
        "\n",
        "model.score(x_test, y_test)"
      ],
      "metadata": {
        "id": "I27qsxndQi5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebe3a91-d772-43ef-9f2a-1a160f1ca592"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9874285714285714"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}